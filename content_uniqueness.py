#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
    Модуль для проверки уникальности контента (ФОТО+ТЕКСТ)
    
    Использует хеширование и DeepSeek AI для проверки семантической схожести
    чтобы гарантировать, что контент никогда не повторяется.
    
    Автор: VR-Lounge
"""

import hashlib
import json
import os
import requests
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# Файл для хранения хешей использованного контента
CONTENT_HASHES_FILE = Path('.content_hashes.json')
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')

def загрузить_хеши_контента() -> Dict:
    """Загружает хеши уже использованного контента"""
    if CONTENT_HASHES_FILE.exists():
        try:
            with open(CONTENT_HASHES_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {'text_hashes': [], 'image_hashes': [], 'content_pairs': []}
    return {'text_hashes': [], 'image_hashes': [], 'content_pairs': []}

def сохранить_хеши_контента(data: Dict):
    """Сохраняет хеши использованного контента"""
    # Ограничиваем размер (храним последние 2000 записей)
    for key in ['text_hashes', 'image_hashes', 'content_pairs']:
        if len(data.get(key, [])) > 2000:
            data[key] = data[key][-2000:]
    
    with open(CONTENT_HASHES_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def создать_хеш_текста(текст: str) -> str:
    """Создаёт хеш текста (нормализованный)"""
    # Нормализуем текст: убираем лишние пробелы, приводим к нижнему регистру
    нормализованный = ' '.join(текст.lower().split())
    # Создаём SHA256 хеш
    return hashlib.sha256(нормализованный.encode('utf-8')).hexdigest()

def создать_хеш_изображения(image_url: str) -> str:
    """Создаёт хеш URL изображения (нормализованный)"""
    # Нормализуем URL: убираем параметры запроса для сравнения
    нормализованный = image_url.split('?')[0].lower().strip()
    return hashlib.sha256(нормализованный.encode('utf-8')).hexdigest()

def создать_хеш_пары(текст: str, image_url: str) -> str:
    """Создаёт хеш пары ФОТО+ТЕКСТ"""
    текст_хеш = создать_хеш_текста(текст)
    изображение_хеш = создать_хеш_изображения(image_url)
    пара = f"{текст_хеш}:{изображение_хеш}"
    return hashlib.sha256(пара.encode('utf-8')).hexdigest()

def проверить_уникальность_текста(текст: str) -> Tuple[bool, Optional[str]]:
    """
    Проверяет, использовался ли уже этот текст
    
    Returns:
        (is_unique, existing_hash) - уникален ли текст и хеш существующего (если есть)
    """
    хеш_текста = создать_хеш_текста(текст)
    данные = загрузить_хеши_контента()
    
    if хеш_текста in данные['text_hashes']:
        return False, хеш_текста
    
    return True, None

def проверить_уникальность_изображения(image_url: str) -> Tuple[bool, Optional[str]]:
    """
    Проверяет, использовалось ли уже это изображение
    
    Returns:
        (is_unique, existing_hash) - уникально ли изображение и хеш существующего (если есть)
    """
    хеш_изображения = создать_хеш_изображения(image_url)
    данные = загрузить_хеши_контента()
    
    if хеш_изображения in данные['image_hashes']:
        return False, хеш_изображения
    
    return True, None

def проверить_уникальность_пары(текст: str, image_url: str) -> Tuple[bool, Optional[str]]:
    """
    Проверяет, использовалась ли уже эта комбинация ФОТО+ТЕКСТ
    
    Returns:
        (is_unique, existing_hash) - уникальна ли пара и хеш существующей (если есть)
    """
    хеш_пары = создать_хеш_пары(текст, image_url)
    данные = загрузить_хеши_контента()
    
    if хеш_пары in данные['content_pairs']:
        return False, хеш_пары
    
    return True, None

def проверить_семантическую_схожесть_через_deepseek(новый_текст: str, существующие_тексты: List[str]) -> Tuple[bool, float]:
    """
    Проверяет семантическую схожесть нового текста с существующими через DeepSeek
    
    Returns:
        (is_similar, similarity_score) - похож ли текст (True если схожесть > 80%)
    """
    if not DEEPSEEK_API_KEY or not существующие_тексты:
        return False, 0.0
    
    try:
        url = "https://api.deepseek.com/v1/chat/completions"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
        }
        
        # Создаём промпт для проверки схожести
        существующие_тексты_кратко = '\n'.join([f"- {t[:200]}..." for t in существующие_тексты[:5]])
        
        system_prompt = """Ты эксперт по анализу текстов. Твоя задача - определить, насколько новый текст похож на существующие тексты по смыслу и содержанию.

Верни ТОЛЬКО JSON в формате:
{
    "similarity_score": число от 0 до 100 (процент схожести),
    "is_similar": true/false (true если схожесть > 80%),
    "reason": краткое объяснение (1 предложение)
}

Схожесть > 80% означает, что тексты слишком похожи и это может быть дубликат."""
        
        user_prompt = f"""Сравни новый текст с существующими текстами:

НОВЫЙ ТЕКСТ:
{новый_текст[:1000]}

СУЩЕСТВУЮЩИЕ ТЕКСТЫ:
{существующие_тексты_кратко}

Определи процент схожести (0-100) и верни JSON."""
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.3,  # Низкая температура для более точного анализа
            "max_tokens": 200
        }
        
        response = requests.post(url, headers=headers, json=data, timeout=30)
        response.raise_for_status()
        
        result = response.json()
        ответ = result['choices'][0]['message']['content']
        
        # Парсим JSON из ответа
        try:
            # Извлекаем JSON из ответа (может быть обёрнут в markdown)
            import re
            json_match = re.search(r'\{[^}]+\}', ответ)
            if json_match:
                parsed = json.loads(json_match.group())
                similarity_score = parsed.get('similarity_score', 0)
                is_similar = parsed.get('is_similar', False) or similarity_score > 80
                return is_similar, similarity_score
        except Exception as e:
            print(f"⚠️ Ошибка парсинга ответа DeepSeek: {e}")
            print(f"Ответ: {ответ}")
        
        return False, 0.0
    
    except Exception as e:
        print(f"⚠️ Ошибка проверки семантической схожести через DeepSeek: {e}")
        return False, 0.0

def проверить_схожесть_заголовков(заголовок1: str, заголовок2: str) -> float:
    """
    Проверяет схожесть двух заголовков по косинусному сходству
    
    Args:
        заголовок1: Первый заголовок
        заголовок2: Второй заголовок
    
    Returns:
        Схожесть от 0.0 до 1.0 (где 1.0 = идентичные заголовки)
    """
    if not заголовок1 or not заголовок2:
        return 0.0
    
    # Нормализуем заголовки
    заголовок1 = заголовок1.lower().strip()
    заголовок2 = заголовок2.lower().strip()
    
    # Если заголовки идентичны
    if заголовок1 == заголовок2:
        return 1.0
    
    # Разбиваем на слова
    слова1 = set(заголовок1.split())
    слова2 = set(заголовок2.split())
    
    # Убираем служебные слова
    служебные_слова = {'для', 'что', 'как', 'и', 'или', 'в', 'на', 'с', 'от', 'до', 
                       'the', 'a', 'an', 'of', 'to', 'in', 'on', 'at', 'for', 'with',
                       'как', 'что', 'для', 'это', 'этот', 'эта', 'это', 'все', 'всего'}
    слова1 -= служебные_слова
    слова2 -= служебные_слова
    
    if not слова1 or not слова2:
        return 0.0
    
    # Косинусное сходство: пересечение / объединение
    пересечение = len(слова1 & слова2)
    объединение = len(слова1 | слова2)
    
    if объединение == 0:
        return 0.0
    
    схожесть = пересечение / объединение
    
    # Дополнительная проверка: если заголовки содержат одинаковые ключевые фразы
    # (например, "мой текущий тренировочный план" и "мой тренировочный план")
    if пересечение >= 3:  # Если минимум 3 слова совпадают
        # Увеличиваем схожесть, если это значимые слова
        схожесть = min(1.0, схожесть * 1.3)
    
    return схожесть

def проверить_полную_уникальность(текст: str, image_url: str, существующие_посты: List[Dict] = None, заголовок: str = None, url_статьи: str = None) -> Tuple[bool, str]:
    """
    Полная проверка уникальности контента (ФОТО+ТЕКСТ+ЗАГОЛОВОК+URL)
    
    Args:
        текст: текст поста
        image_url: URL изображения
        существующие_посты: список существующих постов из blog-posts.json (опционально)
        заголовок: заголовок поста (опционально, для проверки дубликатов)
        url_статьи: URL исходной статьи (опционально, для проверки дубликатов по источнику)
    
    Returns:
        (is_unique, reason) - уникален ли контент и причина (если не уникален)
    """
    # 1. Проверяем уникальность пары ФОТО+ТЕКСТ
    уникальна_пара, хеш_пары = проверить_уникальность_пары(текст, image_url)
    if not уникальна_пара:
        return False, f"Комбинация ФОТО+ТЕКСТ уже использовалась (хеш: {хеш_пары[:16]}...)"
    
    # 2. Проверяем уникальность текста отдельно
    уникален_текст, хеш_текста = проверить_уникальность_текста(текст)
    if not уникален_текст:
        return False, f"Текст уже использовался (хеш: {хеш_текста[:16]}...)"
    
    # 3. Проверяем уникальность изображения отдельно
    уникально_изображение, хеш_изображения = проверить_уникальность_изображения(image_url)
    if not уникально_изображение:
        return False, f"Изображение уже использовалось (хеш: {хеш_изображения[:16]}...)"
    
    # 4. Проверяем схожесть по заголовку и темам (если передан)
    if заголовок and существующие_посты:
        заголовок_нижний = заголовок.lower()
        текст_нижний = текст.lower()
        
        # Список известных персонажей и тем для проверки дубликатов
        известные_персонажи = {
            'brandon': ['brandon', 'брендон', 'брэндон', 'willington'],
            # Можно добавить других персонажей в будущем
        }
        
        # Проверяем, упоминается ли известный персонаж в новом посте
        упомянутые_персонажи = []
        for имя_персонажа, варианты_имени in известные_персонажи.items():
            for вариант in варианты_имени:
                if вариант in заголовок_нижний or вариант in текст_нижний:
                    упомянутые_персонажи.append(имя_персонажа)
                    break
        
        # Проверяем темы (ключевые фразы, которые указывают на одну и ту же тему)
        ключевые_темы = {
            'supercars_systems': ['supercars', 'система', 'системы', 'systems', 'strength'],
            'система_важнее_мотивации': ['система важнее', 'система против мотивации', 'система > мотивации'],
            'гибридный_атлет': ['гибридный атлет', 'hybrid athlete', 'гибридн', 'атлет', 'сила', 'выносливость', 'strong', 'conditioned', 'ready'],
            'питание_до_после_тренировки': ['питание до и после', 'питание до после', 'питание для тренировок', 'что есть до и после', 'питание до тренировки', 'питание после тренировки', 'правильное питание для тренировок'],
        }
        
        упомянутые_темы = []
        for тема, ключевые_слова_темы in ключевые_темы.items():
            совпадения = sum(1 for слово in ключевые_слова_темы if слово in заголовок_нижний or слово in текст_нижний)
            if совпадения >= 2:  # Если минимум 2 ключевых слова темы присутствуют
                упомянутые_темы.append(тема)
        
        for пост in существующие_посты[:50]:  # Проверяем последние 50 постов
            существующий_заголовок = (пост.get('title', '') or '').lower()
            существующий_текст = (пост.get('text', '') or '').lower()
            
            # КРИТИЧЕСКАЯ ПРОВЕРКА: Если упоминается тот же персонаж - это дубликат
            if упомянутые_персонажи:
                for имя_персонажа, варианты_имени in известные_персонажи.items():
                    if имя_персонажа in упомянутые_персонажи:
                        # Проверяем, упоминается ли этот персонаж в существующем посте
                        for вариант in варианты_имени:
                            if вариант in существующий_заголовок or вариант in существующий_текст:
                                return False, f"Статья про того же персонажа уже существует (заголовок: {пост.get('title', '')[:50]}...)"
            
            # КРИТИЧЕСКАЯ ПРОВЕРКА: Если та же тема - это дубликат
            if упомянутые_темы:
                for тема, ключевые_слова_темы in ключевые_темы.items():
                    if тема in упомянутые_темы:
                        совпадения_в_существующем = sum(1 for слово in ключевые_слова_темы if слово in существующий_заголовок or слово in существующий_текст)
                        if совпадения_в_существующем >= 2:
                            return False, f"Статья на ту же тему уже существует (заголовок: {пост.get('title', '')[:50]}...)"
            
            # КРИТИЧЕСКАЯ ПРОВЕРКА: Схожесть заголовков (косинусное сходство)
            схожесть_заголовков = проверить_схожесть_заголовков(заголовок_нижний, существующий_заголовок)
            if схожесть_заголовков > 0.75:  # Порог схожести 75%
                return False, f"Заголовок слишком похож на существующий (схожесть: {схожесть_заголовков:.1%}, заголовок: {пост.get('title', '')[:50]}...)"
            
            # Извлекаем ключевые слова из заголовков
            ключевые_слова_нового = set(заголовок_нижний.split())
            ключевые_слова_существующего = set(существующий_заголовок.split())
            # Убираем служебные слова
            служебные_слова = {'для', 'что', 'как', 'и', 'или', 'в', 'на', 'с', 'от', 'до', 'the', 'a', 'an', 'of', 'to', 'in', 'on', 'at', 'for', 'with'}
            ключевые_слова_нового -= служебные_слова
            ключевые_слова_существующего -= служебные_слова
            
            # Проверяем схожесть по ключевым словам (если больше 50% совпадений - возможен дубликат)
            if ключевые_слова_нового and ключевые_слова_существующего:
                пересечение = ключевые_слова_нового & ключевые_слова_существующего
                if len(пересечение) >= 2:  # Если минимум 2 ключевых слова совпадают
                    # Дополнительная проверка: проверяем первые 500 символов текста
                    новый_текст_начало = текст[:500].lower()
                    существующий_текст_начало = (пост.get('text', '') or '')[:500].lower()
                    # Если начало текста похоже (хотя бы 30% совпадений по ключевым словам)
                    новые_слова_начало = set(новый_текст_начало.split())
                    существующие_слова_начало = set(существующий_текст_начало.split())
                    новые_слова_начало -= служебные_слова
                    существующие_слова_начало -= служебные_слова
                    if новые_слова_начало and существующие_слова_начало:
                        совпадения_текст = len(новые_слова_начало & существующие_слова_начало)
                        минимум_слов = min(len(новые_слова_начало), len(существующие_слова_начало))
                        if минимум_слов > 0 and совпадения_текст / минимум_слов > 0.3:
                            return False, f"Похожая статья уже существует (заголовок: {пост.get('title', '')[:50]}...)"
    
    # 5. КРИТИЧЕСКАЯ ПРОВЕРКА: Проверяем URL источника статьи (чтобы не использовать один и тот же источник повторно)
    if url_статьи and существующие_посты:
        # Нормализуем URL (убираем параметры запроса, trailing slash)
        from urllib.parse import urlparse
        try:
            parsed_url = urlparse(url_статьи)
            нормализованный_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}".rstrip('/').lower()
            
            # Проверяем последние 20 постов на наличие того же URL источника
            for пост in существующие_посты[:20]:
                пост_url = пост.get('source_url') or пост.get('url') or ''
                if пост_url:
                    try:
                        parsed_пост_url = urlparse(пост_url)
                        нормализованный_пост_url = f"{parsed_пост_url.scheme}://{parsed_пост_url.netloc}{parsed_пост_url.path}".rstrip('/').lower()
                        
                        if нормализованный_url == нормализованный_пост_url:
                            return False, f"Статья с этого URL уже была опубликована (заголовок: {пост.get('title', '')[:50]}...)"
                    except:
                        pass
        except Exception as e:
            print(f"⚠️ Ошибка проверки URL источника: {e}")
    
    # 6. Если есть существующие посты, проверяем семантическую схожесть через DeepSeek
    if существующие_посты and DEEPSEEK_API_KEY:
        существующие_тексты = [post.get('text', '') for post in существующие_посты[:20]]  # Проверяем последние 20
        существующие_тексты = [t for t in существующие_тексты if t]  # Убираем пустые
        
        if существующие_тексты:
            похож, score = проверить_семантическую_схожесть_через_deepseek(текст, существующие_тексты)
            if похож:
                return False, f"Текст семантически похож на существующие (схожесть: {score}%)"
    
    return True, "Контент уникален"

def сохранить_контент_как_использованный(текст: str, image_url: str):
    """Сохраняет контент как использованный (добавляет хеши)"""
    данные = загрузить_хеши_контента()
    
    # Добавляем хеши
    хеш_текста = создать_хеш_текста(текст)
    хеш_изображения = создать_хеш_изображения(image_url)
    хеш_пары = создать_хеш_пары(текст, image_url)
    
    if хеш_текста not in данные['text_hashes']:
        данные['text_hashes'].append(хеш_текста)
    
    if хеш_изображения not in данные['image_hashes']:
        данные['image_hashes'].append(хеш_изображения)
    
    if хеш_пары not in данные['content_pairs']:
        данные['content_pairs'].append(хеш_пары)
    
    сохранить_хеши_контента(данные)
    print(f"✅ Контент сохранён как использованный (хеш пары: {хеш_пары[:16]}...)")
